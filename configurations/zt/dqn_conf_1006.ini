[DQN]
### experiment ###
# PRE_TRAIN_MODEL_FILE
GPU_INDEX = 1
PRE_TRAIN_MODEL_FILE = /home/zhangtao/model_file/hello_rl/save/net_c1006_dqn_20180914_210858_20180915_083930.model
# PRE_TRAIN_MODEL_FILE = D:\data\\rl\\model\\net_bko_dqn_20180905_232642_20180907_063819.model
EPOCH_NUM = 360
EPOCH_LENGTH = 30000

### game env ###
# GAME_NAME = riverraid
GAME_NAME = breakout
ACTION_NUM = 4
OBSERVATION_TYPE = image
CHANNEL = 3
WIDTH = 160
HEIGHT = 210
# FRAME_SKIP = 4
FRAME_SKIP = 12

### player ###
TRAIN_PER_STEP = 4

### replay buffer ###
PHI_LENGTH = 4
# PHI_LENGTH = 12
BUFFER_MAX = 100000
# BUFFER_MAX = 200000
BEGIN_RANDOM_STEP = 1000
;BEGIN_RANDOM_STEP = 1000


### q-learning ###
DISCOUNT = 0.90
EPSILON_MIN = 0.15
EPSILON_START = 0.15
;EPSILON_START = 1.0
EPSILON_DECAY = 1
;EPSILON_DECAY = 500000


UPDATE_TARGET_BY_EPISODE_END = 50
UPDATE_TARGET_BY_EPISODE_BEGIN = 50
# update UPDATE_TARGET_DECAY times to get to UPDATE_TARGET_BY_EPISODE_END
UPDATE_TARGET_DECAY = 1

# OPTIMIZER = adagrad
OPTIMIZER = adagrad
LEARNING_RATE = 0.002
WEIGHT_DECAY = 0.0
GRAD_CLIPPING_THETA = 0.01

POSITIVE_REWARD = 0.05
NEGATIVE_REWARD = -1

### OTHER ###
MODEL_PATH = /home/zhangtao/model_file/hello_rl
MODEL_FILE_MARK = c1006_dqn_

EDITED_TIME = 2018-09-15 09:08


;
;[DQN_WITH_MODEL_FILE]
;BEGIN_RANDOM_STEP = 100
;
;EPSILON_MIN = 0.15
;EPSILON_START = 0.15
;EPSILON_DECAY = 1
;
;UPDATE_TARGET_BY_EPISODE_BEGIN = 50
;UPDATE_TARGET_BY_EPISODE_END = 50
;UPDATE_TARGET_DECAY = 1