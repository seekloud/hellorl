# 工作文档

### 目标
1. 完成 q-learning 的基本功能;
2. 在 atari 游戏：'Riverraid' 或 'MsPacman' 中进行实验;
3. 进行一定的调优;


### 步骤
1. 阅读rl相关blog 与 李宏毅rl视频;
2. 阅读github上相关代码 [mxnet实现的q-learning](https://github.com/zmonoid/mxdqn)
3. 设计软件架构;
4. 实现代码, 跑通程序, 完成训练与测试;
5. 调优;


### 主要模块
1. replay store
2. q-network
3. 游戏玩家(player)封装
3. 游戏封装环境(environment)封装
3. 训练(测试)流程封装
4. 结果绘制







### 2018-08-31 16:49
1. 每局开启时增加随机个0操作;
2. 增加梯度裁剪; 
3. update_target_net变间距更新


### 2018-08-31 21:49
1. 修改reward, 将最大值设置为1, 效果很好;
2. 把target net的更新策略也修改了下, 一开始快速更新, 之后减慢更新素的;


### 2018-09-01 20:02
1. 现在有效果, 网络慢慢的学习到了一些东西, lr 0.005
2. 到现在, 训练了23小时, 已经可以平均运行800多个step, 敌人消灭明显;




### 2018-09-02 12:59
1. lr设置为0.02后,效果很不好, 现在改成0.001试试
2. 到现在, 训练了23小时, 已经可以平均运行800多个step, 敌人消灭明显;



### 2018-09-05 17:57
1. game_env step时增加游戏真实得分score的输出




### 一些测试
45M的网络，进行预测，  
GPU，1s，100-120 step  
CPU，1s，30-35 step  


45M的net模型文件  
save，1.21s  
load，0.34s  












